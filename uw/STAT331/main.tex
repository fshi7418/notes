%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Legrand Orange Book
% LaTeX Template
% Version 2.4 (26/09/2018)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com) with modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Compiling this template:
% This template uses biber for its bibliography and makeindex for its index.
% When you first open the template, compile it from the command line with the 
% commands below to make sure your LaTeX distribution is configured correctly:
%
% 1) pdflatex main
% 2) makeindex main.idx -s StyleInd.ist
% 3) biber main
% 4) pdflatex main x 2
%
% After this, when you wish to update the bibliography/index use the appropriate
% command above and make sure to compile with pdflatex several times 
% afterwards to propagate your changes to the document.
%
% This template also uses a number of packages which may need to be
% updated to the newest versions for the template to compile. It is strongly
% recommended you update your LaTeX distribution if you have any
% compilation errors.
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations

\input{structure.tex} % Insert the commands.tex file which contains the majority of the structure behind the template

%\hypersetup{pdftitle={Title},pdfauthor={Author}} % Uncomment and fill out to include PDF metadata for the author and title of the book

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty} % Suppress headers and footers on the title page
\begin{tikzpicture}[remember picture,overlay]
\node[inner sep=0pt] (background) at (current page.center) {\includegraphics[width=\paperwidth]{background.pdf}};
\draw (current page.center) node [fill=ocre!30!white,fill opacity=0.6,text opacity=1,inner sep=1cm]{\Huge\centering\bfseries\sffamily\parbox[c][][t]{\paperwidth}{\centering LM \\[15pt] % Book title
{\Large STAT 331 - Applied Linear Models}\\[20pt] % Subtitle
{\huge Prof. Glen McGee}
}}; % Author name
\end{tikzpicture}
\vfill
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

\noindent Copyright \copyright\ Winter 2021 Prof. Glen McGee\\ % Copyright notice

%\noindent \textsc{Not Published}\\ % Publisher

% \noindent \textsc{book-website.com}\\ % URL

%\noindent Licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License (the ``License''). You may not use this file except in compliance with the License. You may obtain a copy of the License at \url{http://creativecommons.org/licenses/by-nc/3.0}. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \textsc{``as is'' basis, without warranties or conditions of any kind}, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\ % License information, replace this with your own license (if any)
%
%\noindent \textit{First printing, March 2019} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\usechapterimagefalse % If you don't want to include a chapter image, use this to toggle images off - it can be enabled later with \usechapterimagetrue

\chapterimage{chapter_head_1.pdf} % Table of contents heading image

\pagestyle{empty} % Disable headers and footers for the following pages

\tableofcontents % Print the table of contents itself

\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right side of the book

\pagestyle{fancy} % Enable headers and footers again

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

%\part{Part One}

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_2.pdf} % Chapter heading image

\chapter{Review and Simple Linear Regression}

\section{Simple Linear Regression}

\begin{definition} \label{def:111}
\index{Population mean}
\index{Population variance}
\index{Population covariance}
\index{Population correlation}
\index{Sample mean}
\index{Sample variance}
\index{Sample covariance}
\index{Sample correlation}
Let \(X, Y\) be a continuous random variable that models two populations with pdf's \(f_X, f_Y:y\mapsto[0, 1]\) and support \(A\) for \(Y\).
\begin{enumerate}
\item The \textbf{population mean} of \(Y\) is
\[
E(Y) = \int_A yf(y)dy.
\]
\item The \textbf{sample mean} of \(Y\) given a sample \(y_1, \ldots, y_n\) is
\[
\bar{y} = \frac1n\sum_{i=1}^n y_i.
\]
\item The \textbf{population variance} of \(Y\) is
\[
\var(Y) = E((Y - E(Y))^2).
\]
\item The \textbf{sample variance} of \(Y\) given a sample \(y_1, \ldots, y_n\) is
\[
s_y^2 = \frac{1}{n - 1}\sum_{i=1}^n(y_i - \bar{y})^2.
\]
\item The \textbf{population covariance} of \(X\) and \(Y\) is
\[
\cov(X, Y) = E((X - E(X))(Y - E(Y))).
\]
\item The \textbf{sample covariance} for respective samples from the populations modeled by \(X\) and \(Y\), \((x_1, y_1), \ldots, (x_n, y_n)\), is
\[
s_{xy} = \frac{1}{n - 1}\sum_{i=1}^n(y_i - \bar{y})(x_i - \bar{x}).
\]
\item The \textbf{population correlation} between \(X\) and \(Y\) is
\[
\rho_{XY} = \frac{\cov(X, Y)}{\sigma_X\sigma_Y}
\]
where \(\sigma_X = \sqrt{\var(X)}\) and \(\sigma_Y = \sqrt{\var(Y)}\).
\item The \textbf{sample correlation} for observations \((x_1, y_1), \ldots, (x_n, y_n)\) is
\[
r_{xy} = \frac{s_{xy}}{\sqrt{s^2_x s^2_y}}
\]
where \(s_{xy}\) is the sample covariance, \(s^2_x\) is the sample variance of \(x_1, \ldots, x_n\), and \(s^2_y\) is the sample variance of \(y_1, \ldots, y_n\).
\end{enumerate}
\end{definition}

\begin{proposition} \label{prop:112}
\index{Population covariance!Properties}
\begin{enumerate}
\item Suppose \(X, Y\) are rv's, then
\[
\var(Y) = E(Y^2) - E(Y)^2
\]
and
\[
\cov(X, Y) = E(XY) - E(X)E(Y).
\]
\item Suppose \(Y_1, \ldots, Y_m\) are rv's and \(a_i, b_i\), \(1 \leq i \leq m\) are constants, then
\[
E\left(\sum_{i=1}^m a_iY_i + b_i\right) = \sum_{i=1}^ma_iE(Y_i) + \sum_{i=1}^m b_i.
\]
\item Suppose \(Y\) is a rv and \(a, b \in \bR\), then
\[
\var(aY + b) = a^2\var(Y).
\]
\item Suppose \(X, Y\) are rv's and \(a, b, c, d \in \bR\), then
\[
\cov(aY + c, bX + d) = ab\cdot\cov(X, Y).
\]
\item Suppose \(X, Y, U, V\) are rv's, then
\[
\cov(U + V, X + Y) = \cov(U, X) + \cov(X, Y) + \cov(V, X) + \cov(V, Y).
\]
\item Suppose \(X, Y\) are rv's, then
\[
\cov(X, X) = \var(X)
\]
and
\[
\var(X + Y) = \var(X) + \var(Y) + 2\cov(X, Y).
\]
\end{enumerate}
\end{proposition}
\begin{proof}
\begin{enumerate}
\item STAT330(S) Thm. 1.7.6 and STAT330(S) Thm. 2.6.8.
\item STAT330(S) Thm. 1.7.4.
\item STAT330(S) Thm. 1.7.6.
\item For convenience write \(\cov(aY + c, bX + d)\) as \(\cov(Z)\). By (1),
\[
\cov(Z) = E((aY + c)(bX + d)) - E(aY + c)E(bX + d),
\]
which, by (2), yields
\[
\begin{aligned}
\cov(Z) &= E(abXY + cbX + adY + cd) - (aE(Y) + c)(bE(X) + d) \\
&= abE(XY) + bcE(X) + adE(Y) + cd - (aE(Y) + c)(bE(X) + d) \\
&= abE(XY) - abE(X)E(Y) \\
&= ab\cov(X, Y)
\end{aligned}
\]
where the last line comes from (1).
\item By (1),
\[
\begin{aligned}
&\cov(U + V, X + Y) \\
= &E((U + v)(X + Y)) - E(U + V)E(X + Y) \\
= &E(UX + UY + VX + VY) - (E(U) + E(V))(E(X) + E(Y)) \\
= &E(UX) - E(U)E(X) + (E(UY) - E(U)E(Y)) + (E(VX) - E(V)E(X)) + (E(VY) - E(V)E(Y)) \\
= &\cov(U, X) + \cov(U, Y) + \cov(V, X) + \cov(V, Y)
\end{aligned}
\]
where the last line again comes from (1).
\item STAT330(S) Thm. 2.6.10.
\end{enumerate}
\end{proof}

\begin{definition} \label{def:113}
\index{Gamma function}
The \textbf{Gamma function} is
\[
\begin{aligned}
\Gamma: (0, \infty) &\rightarrow \bR \\
\alpha &\mapsto \int_0^\infty y^{\alpha - 1}e^{-y}dy
\end{aligned}
\]
\end{definition}

\begin{proposition} \label{prop:114}
Let \(\alpha > 1\), \(n \in \bN\), then
\begin{enumerate}
\item \(\Gamma(\alpha) = (\alpha - 1)\Gamma(\alpha - 1)\).
\item \(\Gamma(n) = (n - 1)!\).
\item \(\Gamma\left(\frac12\right) = \sqrt\pi\).
\end{enumerate}
\end{proposition}
\begin{proof} STAT330(S) Proposition 1.4.6.\end{proof}

\begin{definition} \label{def:115}
\index{Normal distribution}
\index{Chi-squared distribution}
\index{Student \(t\)-distribution}
\begin{enumerate}
\item Let \(\mu, \sigma \in \bR\), \(Z\) be a continuous rv with pdf
\[
f(z) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(z - \mu)^2}{2\sigma^2}\right), z \in \bR,
\]
then \(Z\) is said to follow a \textbf{normal distribution}, written as \(Z \sim N(\mu, \sigma^2)\).
\item Let \(k \in \bN\) and \(X\) be a continuous rv with pdf
\[
f(x) = \frac{1}{2^{\frac{k}{2}}\Gamma\left(\frac k 2\right)} x^{\frac{k}{2} - 1} e^{-\frac{x}{2}}, x > 0,
\]
then \(X\) is said to follow a \textbf{chi-squared distribution with \(k\) degrees of freedom}, written as \(X \sim \chi^2_k\) or sometimes \(X \sim \chi^2(k)\).
\item Let \(k \in \bN\), and suppose \(Y\) is a continuous rv with pdf
\[
f(y) = \frac{\Gamma\left(\frac{k+1}{2}\right)}{\sqrt{k\pi}\Gamma\left(\frac{k}{2}\right)}\left(1 + \frac{y^2}{k}\right)^{-\frac{k+1}{2}}, y \in \bR,
\]
then \(Y\) is said to follow a \textbf{student \(t\)-distribution with \(k\) degrees of freedom}, and we write \(Y \sim t_k\) or sometimes \(Y \sim t(k)\).
\end{enumerate}
\end{definition}

\begin{proposition} \label{prop:116}
\begin{enumerate}
\item If \(Z \sim N(\mu, \sigma^2)\), then
\[
E(Z) = \mu, \var(Z) = \sigma^2.
\]
\item If \(X \sim \chi^2_k\) for some \(k \in \bN\), then
\[
E(X) = k, \var(X) = 2k.
\]
\item If \(Y \sim t_k\) for some \(k \in \bN\), then
\[
E(Y) = 0 \IF k = 2, 3, \ldots
\]
\[
\var(Y) = \frac{k}{k - 2} \IF k = 3, 4, \ldots
\]
and otherwise \(E(Y)\) and \(\var(Y)\) do not exist.
\end{enumerate}
\end{proposition}
\begin{proof} We omit the proof.\end{proof}

\begin{proposition} \label{prop:117}
\begin{enumerate}
\item If \(Z_i \sim N(\mu_i, \sigma_i^2)\), \(\alpha_i \in \bR\), \(1 \leq i \leq n\), then
\[
U := \sum_{i=1}^n \alpha_iZ_i \sim N\left(\sum_{i=1}^n\alpha_i\mu_i, \sum_{i=1}^n\alpha_i^2\sigma_i^2\right).
\]
\item If \(Z_i \sim N(0, 1)\) independently, \(1 \leq i \leq n\), then
\[
X := \sum_{i=1}^n Z_i^2 \sim \chi^2_n.
\]
\item If \(Z \sim N(0, 1)\) and \(X \sim \chi^2_k\) independently for some \(k \in \bN\), ten
\[
\frac{Z}{\sqrt{\frac{X}{k}}} \sim t_k.
\]
\end{enumerate}
\end{proposition}
\begin{proof}
\begin{enumerate}
\item STAT330(S) Thm. 3.3.2.
\item STAT330(S) Proposition. 3.3.5.
\item STAT330(S) Lemma. 3.3.9.
\end{enumerate}
\end{proof}

\begin{remark} \label{rmk:118}
\index{Independent variable}
\index{Dependent variable}
In regression, we are interested in predicting an observed random variable \(y\) given an observed random variable \(x\). To quantify any relationships between \(x\) and \(y\), we may use simple linear regression. \\
\indent We use the convention that \(y\) is the variable we are interested in predicting, and \(x\) is the ``given'' observed random variable. \\
\indent We usually call \(y\) to be the \textbf{independent variable} and \(x\) to be the \textbf{dependent variable}.
\end{remark}

\begin{definition} \label{def:119}
\index{Simple linear regression}
\index{Simple linear regression!Error terms}
Let \(Y := (Y_1, \ldots, Y_n)\) be a random vector and \(x := (x_1, \ldots, x_n)\) be an observed random vector that represents dependent variables. A \textbf{simple linear regression} is a group of \(n\) functions
\[
Y_i = \beta_0 + \beta_1x_i + \ep_i, 1 \leq i \leq n
\]
for some unobserved \(\beta_0, \beta_1 \in \bR\) and \(\ep_i \sim N(0, \sigma^2)\) (called \textbf{error terms}) independently (with respect to \(i\)) for some unobserved \(\sigma^2\). \\
\indent In matrix form, a simple linear regression is written as
\[
Y = \beta_0 + \beta_1x + \ep.
\]
\end{definition}

\begin{remark} \label{rmk:1110}
\index{Simple linear regression!Assumptions}
\index{Homoskedasticity}
As a consequence of the above definition, the random observations \(Y_i\) has the distribution
\[
Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2)
\]
independently. This in turn yields
\[
E(Y_i|x_i) = \beta_0 + \beta_1x_i.
\]
\indent The underlying assumptions of this model are
\begin{enumerate}
\item linearity between \(X\) and \(Y\)
\item independent distribution of the error terms
\item normality of the error terms distribution
\item (homoskedasticity) equal variance of \(Y_i\)'s with respect to \(i\).
\end{enumerate}
\end{remark}

\begin{theorem} \label{thm:1111}
\index{Simple linear regression!Maximum likelihood estimator}
Let \(X, Y\) be rv's and assume a simple linear regression model
\[
Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2),
\]
then the maximum likelihood estimates for \(\beta_0\), \(\beta_1\), and \(\sigma^2\) are
\[
\hat{\beta}_1 = \frac{s_{xy}}{s_{xx}},
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x},
\hat{\sigma}^2 = \frac1n\sum_{i=1}^n(y_i - (\hat{\beta}_0 + \hat{\beta}_1x_i))^2
\]
where \((x_i, y_i)\), \(1 \leq i \leq n\), is an observed sample of size \(n\).
\end{theorem}
\begin{proof} We omit the proof.\end{proof}

\begin{definition} \label{def:1112}
\index{Simple linear regression!Fitted value}
\index{Simple linear regression!Residual}
Let \(X, Y\) be rv's, \((x_i, y_i)\), \(1 \leq i \leq n\) be observations, and assume a simple linear regression model \(Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2)\). Let \(\hat{\beta}_0\), \(\hat{\beta}_1\) be estimates for \(\beta_0, \beta_1\), then the \textbf{fitted values} of the model are
\[
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i, 1 \leq i \leq n
\]
and the \textbf{residuals} of the model are 
\[
E_i := Y_i - \hat{y}_i, 1 \leq i \leq n.
\]
\end{definition}

\begin{proposition} \label{prop:1113}
Let \(X, Y\) be ev's with observations \((x_i, y_i)\), \(1 \leq i \leq n\) and assume a simple linear regression model \(Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2)\), then
\[
\frac{1}{\sigma^2}\sum_{i=1}^n E_i^2 \sim \chi^2_{n - 2},
\]
where \(E_i\)'s are residuals.
\end{proposition}
\begin{proof} We omit the proof.\end{proof}

\begin{corollary} \label{cor:1114}
With the same setup as above, the estimator
\[
\tilde{\sigma}^2 = \frac{1}{n - 2}\sum_{i=1}^n E_i^2
\]
for \(\sigma^2\) is unbiased.
\end{corollary}
\begin{proof} We have, by \hyperref[prop:1118]{Proposition 1.1.12} and the expected value of the \(\chi^2\) distribution,
\[
E(\tilde{\sigma}^2) = \sigma^2E\left(\frac{1}{\sigma^2(n - 2)}\sum_{i=1}^nE_i^2\right) = \frac{\sigma^2}{n - 2}E\left(\frac{1}{\sigma^2}\sum_{i=1}^nE_i^2\right) = \frac{\sigma^2}{n - 2}(n - 2) = \sigma^2.
\]
\indent This completes the proof.
\end{proof}

\begin{definition} \label{def:1115}
\index{Least squared error}
\index{LSE|see{Least squared error}}
\index{Sum squared error}
\index{SSE|see{Sum squared error}}
Let \(X, Y\) be rv's with observations \((x_i, y_i)\), \(1 \leq i \leq n\), modelled by a simple linear regression. The \textbf{least squared error(LSE) estimates} for parameters \(\beta_0\), \(\beta_1\) are \(\hat{\beta}_0\), \(\hat{\beta}_1\) such that they minimise the \textbf{sum squared error (SSE)}
\[
S(\beta_0, \beta_1) := \sum_{i=1}^n (y_i - (\beta_0 + \beta_1x_i))^2.
\]
\end{definition}

\begin{theorem} \label{thm:1116}
In a simple linear regression, the LSE estimators are equivalent to the maximum likelihood estimators for the parameters \(\beta_0\) and \(\beta_1\).
\end{theorem}
\begin{proof} For convenience, all \(\sum\) are with respcet to \(i\) from \(i = 1\) to \(n\). \\
\indent Setting
\[
\frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial\hat{\beta}_0} = 0 = \frac{\partial S(\hat{\beta}_0, \hat{\beta}_1)}{\partial\hat{\beta}_1}
\]
simultaneously yields
\[
\left\{
\begin{array}{rl}
0 &= \sum 2(y_i - (\hat{\beta}_0 + \hat{\beta}_1x_i))(-1) \\
0 &= \sum 2(y_i - (\hat{\beta}_0 + \hat{\beta}_1x_i))(-x_i)
\end{array}
\right.
\]
\[
\left\{
\begin{array}{rl}
0 &= \sum y_i - n\hat{\beta}_0 - \hat{\beta}_1 \sum x_i \\
0 &= \sum (y_ix_i - \hat{\beta}_0x_i - \hat{\beta}_1x_i^2)
\end{array}
\right.
\]
\[
\left\{
\begin{array}{rl}
\hat{\beta}_0 &= \frac1n\sum y_i - \hat{\beta}_1\frac1n\sum x_i \\
0 &= \sum y_ix_i - \hat{\beta}_0n\bar{x} - \hat{\beta}_1 \sum x_i^2
\end{array}
\right.
\]
\[
\left\{
\begin{array}{rl}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1\bar{x} \\
0 &= \sum y_ix_i - (y - \hat{\beta}_1\bar{x})\bar{x} - \hat{\beta}_1\sum x_i^2
\end{array}
\right.
\]
\[
\left\{
\begin{array}{rl}
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1\bar{x} \\
\hat{\beta}_1 &= \frac{\sum y_ix_i - n\bar{x}\bar{y}}{\sum x_i^2 - n\bar{x}^2}
\end{array}
\right.
\]
\indent Immediately we get equivalence of \(\hat{\beta}_0\) with \hyperref[thm:1111]{Thm. 1.1.11}. For \(\hat{\beta}_1\), first we observe that
\[
\sum\bar{y}(x_i - \bar{x}) = \bar{y}(\sum x_i - n\bar{x}) = \bar{y}\cdot0 = 0
\]
and similarly \(\sum\bar{x}(x_i - \bar{x}) = 0\). \\
\indent Now the numerator of \(\hat{\beta}_1\) is
\[
\sum y_ix_i - n\bar{x}\bar{y} = \sum y_ix_i - n\bar{x}\frac1n\sum y_i = \sum y_i(x_i - \bar{x}).
\]
\indent Because \(\sum\bar{y}(x_i - \bar{x}) = 0\), we can add this term to the numerator for free to get the numerator to be
\[
\sum(y_i - \bar{y})(x_i - \bar{x}) = (n - 1)s_{xy}.
\]
\indent Similarly, for the denominator, we have
\[
\begin{aligned}
&\sum x_i^2 - n\bar{x}^2 = \sum x_i^2 - n\bar{x}\frac1n\sum x_i = \sum x_i(x_i - \bar{x}) = \sum x_i(x_i - \bar{x}) - 0 \\
= &\sum x_i(x_i - \bar{x}) - \sum\bar{x}(x_i - \bar{x}) = \sum(x_i - \bar{x})^2 = (n - 1)s_{xx}
\end{aligned}
\]
\indent Thus 
\[
\hat{\beta}_1 = \frac{(n - 1)s_{xy}}{(n - 1)s_{xx}} = \frac{s_{xy}}{s_{xx}},
\]
which corresponds to \hyperref[thm:1111]{Thm. 1.1.11} as well.
\end{proof}

\begin{remark} \label{rmk:1117}
The MLE is the LSE estimators in a simple linear regression model may not agree if the normality assumption in \hyperref[rmk:1110]{Remark 1.1.10} is violated. \\
\indent However we do continue to hold this assumption going forward unless otherwise stated, and when we write \(\hat{\beta}_0\) and \(\hat{\beta}_1\), we just mean the LSE/MLE estimate.
\end{remark}

\begin{proposition} \label{prop:1118}
Let \(X, Y\) be rv's and assume a simple linear regression model
\[
Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2).
\]
Define, for \(1 \leq i \leq n\),
\[
w_i = \frac{X_i - \bar{x}}{\sum_{i=1}^n(x_i - \bar{x})^2},
\]
then
\[
\tilde{\beta}_1 \sim N\left(\sum_{i=1}^n w_i(\beta_0 + \beta_1x_i), \sigma^2\sum_{i=1}^n w_i^2\right).
\]
\end{proposition}
\begin{proof} From \hyperref[thm:1116]{Thm. 1.1.16} we get
\[
\tilde{\beta}_1 = \frac{S_{XY}}{s_{xx}} = \frac{\sum_{i=1}^n(Y_i - \bar{Y})(x_i - \bar{x})}{\sum_{i=1}^n(x_i - \bar{x})^2|} = \frac{\sum_{i=1}^n Y_i(x_i - \bar{x})}{\sum_{i=1}^n(x_i - \bar{x})^2} = \sum_{i=1}^n w_iY_i.
\]
\indent Now each \(Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2)\), so via \hyperref[prop:117]{Proposition 1.1.7} we get
\[
\tilde{\beta}_1 \sim N\left(\sum_{i=1}^nw_i(\beta_0 + \beta_1x_i), \sigma^2\sum_{i=1}^n w_i^2\right).
\]
\end{proof}

\begin{corollary} \label{cor:1119}
With the same setup as above, the estimator
\[
\tilde{\beta}_1 = \frac{S_{XY}}{s_{xx}}
\]
is unbiased.
\end{corollary}
\begin{proof} By \hyperref[prop:1118]{Proposition 1.1.18} we immeidately get
\[
E(\tilde{\beta}_1) = \sum_{i=1}^nw_i(\beta_0 + \beta_1x_i).
\]
Substituting in \(w_i\) yields
\[
\begin{aligned}
E(\tilde{\beta}_1) &= \sum_{i=1}^n\frac{x_i - \bar{x}}{\sum_{i=1}^n(x_i - \bar{x})^2}(\beta_0 + \beta_1x_i) \\
&= \beta_0\sum_{i=1}^n\frac{x_i - \bar{x}}{\sum_{i=1}^n(x_i - \bar{x})^2} + \beta_1\sum_{i=1}^n\frac{(x_i - \bar{x})x_i}{\sum_{i=1}^n(x_i - \bar{x})^2} \\
&= \frac{\beta_0}{\sum_{i=1}^n(x_i - \bar{x})^2}\sum_{i=1}^n(x_i - \bar{x}) + \beta_1\sum_{i=1}^n\frac{(x_i - \bar{x})(x_i - \bar{x})}{\sum_{i=1}^n(x_i - \bar{x})^2} \\
&= \frac{\beta_0}{\sum_{i=1}^n(x_i - \bar{x})^2}\cdot0 + \beta_1 \\
&= \beta_1
\end{aligned}
\]
as required.
\end{proof}

\begin{corollary} \label{cor:1120}
With the same setup as \hyperref[prop:1118]{Proposition 1.1.18}, 
\[
\var(\tilde{\beta}_1) = \frac{\sigma^2}{s_{xx}} = \frac{\sigma^2}{\sum_{i=1}^n(x_i - \bar{x})^2}.
\]
\end{corollary}
\begin{proof} Again, directly from \hyperref[prop:1118]{Proposition 1.1.18} we have
\[
\var(\tilde{\beta}_1) = \sigma^2\sum_{i=1}^n w_i^2 = \sigma^2\sum_{i=1}^n\frac{(x_i - \bar{x})^2}{\left(\sum_{i=1}^n (x_i - \bar{x})^2\right)^2} = \frac{\sigma^2}{\sum_{i=1}^n(x_i - \bar{x})^2} = \frac{\sigma^2}{s_{xx}}.
\]
as required.
\end{proof}

\begin{proposition} \label{prop:1121}
With the same setup as \hyperref[prop:1118]{Proposition 1.1.18}, 
\[
\tilde{\beta}_0 \sim N\left(\beta_0, \sigma\left(\frac1n + \frac{\bar{x}^2}{s_{xx}}\right)\right),
\]
in particular, \(\tilde{\beta}_0\) is an unbiased estimator of \(\beta_0\).
\end{proposition}
\begin{proof} From the proof of \hyperref[thm:1116]{Thm. 1.1.16}, \(\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}\). Now each
\[
Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2), 1 \leq i \leq n,
\]
which means
\[
\bar{Y} \sim N\left(\beta_0 + \beta_1\bar{x}, \frac{\sigma^2}{n}\right).
\]
\indent Thus, along with the fact that \(E(\tilde{\beta}_1) = \beta_1\), we have
\[
E(\tilde{\beta}_0) = E(\bar{Y}) - \bar{x}E(\tilde{\beta}_1) = \beta_0 + \beta_1\bar{x} - \bar{x}\beta_1 = \beta_0.
\]
\indent Next,
\[
\var(\tilde{\beta}_0) = \var(\bar{Y} - \tilde{\beta}_1\bar{x}) = \var(\bar{Y}) + \bar{x}^2\var(\tilde{\beta}_1) + 2(-\bar{x})\cov(\bar{Y}, \tilde{\beta}_1),
\]
where
\[
\begin{aligned}
\cov(\bar{Y}, \tilde{\beta}_1) &= \cov\left(\frac1n\sum_iY_i, \frac{\sum_iY_i(x_i - \bar{x})}{\sum_i(x_i - \bar{x})^2}\right) \\
&= \frac{1}{n\sum_i(x_i - \bar{x})^2}\cov\left(\sum_iY_i, \sum_jY_j(x_j - \bar{x})^2\right) \\
&= \frac{1}{n\sum_i(x_i - \bar{x})^2}\sum_j(x_j - \bar{x})\sum_i\cov(Y_i, Y_j) \\
&= \frac{1}{n\sum_i(x_i - \bar{x})^2}\sum_j(x_j - \bar{x})\sigma^2 \\
&= 0
\end{aligned}
\]
and \(\var(\bar{Y}) = \frac{\sigma^2}{n}\), \(\var(\tilde{\beta}_1) = \sigma^2\sum_iw_i^2\). So
\[
\begin{aligned}
\var(\tilde{\beta}_0) &= \frac{\sigma^2}{n} + \bar{x}^2\sigma^2\sum_iw_i^2 \\
&= \frac{\sigma^2}{n} + \bar{x}^2\sigma^2\sum_i\frac{(x_i - \bar{x})^2}{\left(\sum_i(x_i - \bar{x})^2\right)^2} \\
&= \frac{\sigma^2}{n} + \bar{x}^2\sigma^2\frac{1}{\sum_i(x_i - \bar{x})^2} \\
&= \sigma^2\left(\frac1n + \frac{\bar{x}^2}{s_{xx}}\right)
\end{aligned}
\]
\indent Finally, \(\tilde{\beta}_0\) follows a normal distribution because it is a linear combination of normal distributions, thus
\[
\tilde{\beta}_0 \sim N\left(\beta_0, \sigma^2\left(\frac1n + \frac{\bar{x}^2}{s_{xx}}\right)\right)
\]
as required.
\end{proof}

\begin{proposition} \label{prop:1122}
\index{Simple linear regression!Confidence interval}
Let \(X, Y\) be rv's and assume a simple linear regression model
\[
Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2), 1\leq i \leq n.
\]
\indent Fix \(q \in [0, 1]\).
\begin{enumerate}
\item If \(\sigma^2\), the population variance, is known, then the \(100q\%\) confidence interval of the parameter \(\beta_1\) is
\[
\left[\hat{\beta}_1 - a\frac{\sigma}{\sqrt{s_{xx}}}, \hat{\beta}_1 + a\frac{\sigma}{\sqrt{s_{xx}}}\right]
\]
where \(a = \Phi^{-1}\left(1 - \frac12(1 - q)\right)\), \(\Phi\) is the distribution function of the standard normal distribution.
\item If the population variance is unknown, then the \(100q\%\) confidence interval of \(\beta_1\) is
\[
\left[\hat{\beta}_1 - b\frac{\hat{\sigma}}{\sqrt{s_{xx}}}, \hat{\beta}_1 + b\frac{\hat{\sigma}}{\sqrt{s_{xx}}}\right]
\]
where \(b = \Psi_{n - 2}^{-1}\left(1 - \frac12(1 - q)\right)\), \(n\) is the sample size, \(\Psi_{n-2}\) is the distribution function of the student \(t\)-distribution with \(n - 2\) degrees of freedom, and 
\[
\hat{\sigma}^2 = \frac{1}{n - 2}\sum_{i=1}^ne_i^2
\]
is the sample estimator of \(\sigma^2\).
\end{enumerate}
\end{proposition}
\begin{proof} We know that
\[
\tilde{\beta}_1 \sim N\left(\beta_1, \frac{\sigma^2}{s_{xx}}\right)
\]
and so \(\frac{\tilde{\beta}_1 - \beta_1}{\sigma/\sqrt{s_{xx}}} \sim N(0, 1)\). \\
\indent Meanwhile, we know that, from \hyperref[prop:1113]{Proposition 1.1.13}, 
\[
\frac{1}{\sigma^2}\sum_{i=1}^ne_i^2 \sim \chi^2_{n - 2}.
\]
\indent Write
\[
\frac{\tilde{\beta}_1 - \beta_1}{\hat{\sigma}/\sqrt{s_{xx}}} = \frac{\tilde{\beta}_1 - \beta_1}{\sqrt{\frac{1}{n - 2}\sum_{i=1}^ne_i^2}/\sqrt{s_{xx}}} = \frac{\sigma}{\sigma}\cdot\frac{(\tilde{\beta}_1 - \beta_1)/(1 / \sqrt{s_{xx}})}{\sqrt{\frac{1}{n - 2}\sum_{i=1}^ne_i^2}} = \frac{(\tilde{\beta}_1 - \beta_1) / (\sigma / \sqrt{s_{xx}})}{\sqrt{\frac{1}{\sigma^2(n - 2)}\sum_{i=1}^ne_i^2}}.
\]
\indent Denote \(Z := (\tilde{\beta}_1 - \beta_1) / (\sigma / \sqrt{s_{xx}})\) and \(V := \frac{1}{\sigma^2}\sum_{i=1}^ne_i^2\), we have \(Z \sim N(0, 1)\) and \(V \sim \chi^2_{n - 2}\). By \hyperref[prop:117]{Proposition 1.1.7}(3), we have
\[
\frac{\tilde{\beta}_1 - \beta_1}{\hat{\sigma} / \sqrt{s_{xx}}} = \frac{Z}{\sqrt{V / (n - 2)}} \sim t_{n - 2}.
\]
We now have everything we need for the actual proof concerning the confidence intervals.
\begin{enumerate}
\item Set \(a = \Phi^{-1}(1 - \frac12(1 - q))\). By the symmetry of the normal distribution, we get
\[
\Pr\left(-a \leq \frac{\tilde{\beta}_1 - \beta_1}{\sigma / \sqrt{s_{xx}}} \leq a\right) = q
\]
or equivalently
\[
q = \Pr\left(\tilde{\beta}_1 - a\frac{\sigma}{\sqrt{s_{xx}}} \leq \beta_1 \leq \tilde{\beta}_1 + a\frac{\sigma}{\sqrt{s_{xx}}}\right).
\]
\indent By the definition of a confidence interval, 
\[
\left[\hat{\beta}_1 - a\frac{\sigma}{\sqrt{s_{xx}}}, \hat{\beta}_1 + a\frac{\sigma}{\sqrt{s_{xx}}}\right]
\]
is a \(100q\%\) confidence interval of \(\beta_1\).
\item Set \(b = \Psi^{-1}_{n - 2}\left(1 - \frac12(1 - q)\right)\). By the symmetry of the student \(t\)-distribution,
\[
q = \Pr\left(-b \leq \frac{\tilde{\beta}_1 - \beta_1}{\hat{\sigma} / \sqrt{s_{xx}}} \leq b\right)
\]
or equivalently
\[
q = \Pr\left(\tilde{\beta}_1 - b\frac{\sigma}{\sqrt{s_{xx}}} \leq \beta_1 \leq \tilde{\beta}_1 + b\frac{\sigma}{\sqrt{s_{xx}}}\right).
\]
\indent By the definition of a confidence interval, 
\[
\left[\hat{\beta}_1 - b\frac{\sigma}{\sqrt{s_{xx}}}, \hat{\beta}_1 + b\frac{\sigma}{\sqrt{s_{xx}}}\right]
\]
is a \(100q\%\) confidence interval of \(\beta_1\).
\end{enumerate}
\end{proof}

%------------------------------------------------

\section{Hypothesis Testing in Simple Linear Regression}

\begin{remark} \label{rmk:121}
\index{Simple linear regression!Hypothesis testing}
Let \(X, Y\) be rv's and suppose \((x_i, y_i)\), \(1 \leq i \leq n\) are sample observations. Assume a simple linear regression model
\[
Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2)
\]
and suppose \(\hat{\beta}_0\), \(\hat{\beta}_1\) are LSE based on the observations. \\
\indent Let \(\theta_0 \in \bR\). We are interested in testing the simple hypothesis
\[
H_0: \beta_1 = \theta_0,
\]
The ``evidence'' for the hypothesis test is quantified by a probability: the probability of observing \(\hat{\beta}_1\) and \(\hat{\sigma}\) if \(H_0\) is true. The lower the probability is , the stronger the evidence against \(H_0\) is. \\
\indent The convention in linear regression hypothesis testing is to test \(H_0: \beta_1 = 0\), i.e. no linear relationship between \(X\) and \(Y\). \\
\indent We already know that
\[
T := \frac{\hat{\beta}_1 - \beta_1}{\hat{\sigma} / \sqrt{s_{xx}}} \sim t_{n - 2}.
\]
\indent The probability of observing \((x_i, y_i)\), \(1 \leq i \leq n\), or something even more extreme, given \(\beta_1 = 0\) is
\[
\Pr\left(|T| > \left|\frac{\hat{\beta}_1}{\hat{\sigma} / \sqrt{s_{xx}}}\right|\right) = 2\Pr\left(T \geq \left|\frac{\hat{\beta}_1}{\hat{\sigma} / \sqrt{s_{xx}}}\right|\right), T \sim t_{n - 2}.
\]
\end{remark}

%------------------------------------------------

\section{Prediction of a Simple Linear Regression Model}

\begin{definition} \label{def:131}
\index{Simple linear regression!Mean response}
\index{Simple linear regression!Prediction}
Let \(X, Y\) be rv's with observations \((x_i, y_i)\), \(1 \leq i \leq n\). Assume a simple linear regression model
\[
Y_i = \beta_0 + \beta_1 + \ep_i, 1\leq i \leq n
\]
and let \(\hat{\beta}_0\), \(\hat{\beta}_1\) be LSEs for \(\beta_0\) and \(\beta_1\). \\
\indent The \textbf{mean response} of this model given an arbitrary input \(x_0\) is
\[
\mu_0 = \beta_0 + \beta_1x_0.
\]
\indent The \textbf{predicted value} of \(Y\) given a new observation \(x_{new}\) based on this model is
\[
y_{new} = \beta_0 + \beta_1x_{new}.
\]
\end{definition}

\begin{proposition} \label{prop:132}
With the same setup as \hyperref[def:131]{Def. 1.3.1}, the mean response LSE
\[
\tilde{\mu}_0 = \tilde{\beta}_0 + \tilde{\beta}_1x_0
\]
follows the distribution
\[
N\left(\mu_0, \sigma^2\left(\frac1n + \frac{(x_0 - \bar{x})^2}{\sum_{i=1}^n(x_i - \bar{x})^2}\right)\right) = N\left(\beta_0 + \beta_1x_0, \sigma^2\left(\frac1n + \frac{(x_0 - \bar{x})^2}{\sum_{i=1}^n(x_i - \bar{x})^2}\right)\right).
\]
\end{proposition}
\begin{proof} By the linearity of expectation, \hyperref[cor:1119]{Corollary 1.1.19}, and \hyperref[prop:1121]{Proposition 1.1.20}, we have
\[
E(\tilde{\mu}_0) = E(\tilde{\beta}_0) + x_0E(\tilde{\beta}_1) = \beta_0 + \beta_1x_0 = \mu_0.
\]
\indent On the other hand
\[
\var(\tilde{\mu}_0) = \var(\tilde{\beta}_0 + \tilde{\beta}_1x_0) = \var(\bar{Y} - \tilde{\beta}_1\bar{x} + \tilde{\beta}_1x_0) = \var(\bar{Y} + \tilde{\beta}_1(x_0 - \bar{x})).
\]
\indent Writing is out yields
\[
\var(\tilde{\mu}_0) = \var\left(\frac1n\sum_{i=1}^nY_i + \left(\sum_{i=1}^n\frac{x_i - \bar{x}}{s_{xx}}y_i\right)(x_0 - \bar{x})\right) = \var\left(\sum_{i=1}^n\left(\frac1n + \frac{(x_i - \bar{x})(x_0 - \bar{x})}{s_{xx}}\right)Y_i\right).
\]
\indent By model assumption, each \(\var(Y_i) = \sigma^2\). Moreover the \(Y_i\)'s are assumed to be independent, so by \hyperref[prop:117]{Proposition 1.1.7}, 
\[
\begin{aligned}
\var(\tilde{\mu}_0) &= \sum_{i=1}^n\left(\frac1n + \frac{(x_i - \bar{x})(x_0 - \bar{x})}{s_{xx}}\right)^2\sigma^2 \\
&= \sigma^2\sum_{i=1}^n\left(\frac{1}{n^2} + \frac2n\left(\frac{(x_i - \bar{x})(x_0 - \bar{x})}{s_{xx}}\right) + \frac{(x_i - \bar{x})^2(x_0 - \bar{x})^2}{s_{xx}^2}\right) \\
&= \sigma^2\left(\frac{n}{n^2} + \frac{2(x_0 - \bar{x})}{s_{xx}}\sum_{i=1}^n(x_i - \bar{x}) + \frac{(x_0 - \bar{x})^2}{s_{xx}^2}\sum_{i=1}^n(x_i - \bar{x})^2\right) \\
&= \sigma^2\left(\frac1n + 0 + \frac{(x_0 - \bar{x})^2}{s_{xx}^2}s_{xx}\right) \\
&= \sigma^2\left(\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}\right)
\end{aligned}
\]
where \(s_{xx} = \sum_{i=1}^n(x_i - \bar{x})^2\). \\
\indent Finally, the above shows that \(\tilde{\mu}_0\) can be written as a linear combination of \(Y_i\)'s, all of which are normally distributed, so
\[
\tilde{\mu}_0 \sim N\left(\mu_0, \sigma^2\left(\frac1n + \frac{(x_0 - \bar{x})^2}{\sum_{i=1}^n(x_i - \bar{x})^2}\right)\right)
\]
as required.
\end{proof}

\begin{proposition} \label{prop:133}
\index{Simple linear regression!Confidence interval}
With the same setup as \hyperref[def:131]{Def. 1.3.1}, \(\alpha \in [0, 1)\), and known parameter \(\sigma^2\), the \(100\alpha\%\) confidence interval of \(\mu_0\) is
\[
\left[\hat{\mu}_0 - a\sigma\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}, \hat{\mu}_0 + a\sigma\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}\right]
\]
where \(a = \Phi^{-1}\left(\frac12(1 + \alpha)\right)\), \(\Phi\) being the distribution function of the standard normal distribution.
\end{proposition}
\begin{proof} The arguments are analogous to \hyperref[prop:1122]{Proposition 1.1.22}(1). \end{proof}

\begin{proposition} \label{prop:134}
\index{Simple linear regression!Confidence interval}
With the same setup as \hyperref[def:131]{Def. 1.3.1}, \(\alpha \in [0, 1)\), and unknown population variance \(\sigma^2\), the \(100\alpha\%\) confidence interval of \(\mu_0\) is
\[
\left[\hat{\mu}_0 - q_\alpha\hat{\sigma}\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}, \hat{\mu}_0 + q_\alpha\hat{\sigma}\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}\right]
\]
where
\[
\hat{\sigma} = \frac{1}{n - 2}\sum_{i=1}^ne_i^2
\]
is the variance estimate, and \(q_\alpha = \Psi^{-1}_{n - 2}\left(1 - \frac12(1 - \alpha)\right)\), \(\Phi_{n - 2}\) being the distribution function of the \(t_{n-2}\) distribution.
\end{proposition}
\begin{proof} From \hyperref[prop:132]{Proposition 1.3.2},
\[
\frac{\tilde{\mu}_0 - \mu_0}{\sigma\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}} \sim N(0, 1).
\]
\indent Now by \hyperref[prop:1113]{Proposition 1.1.13}, \(\frac{1}{\sigma^2}\sum_{i=1}^nE_i^2 \sim \chi^2_{n - 2}\), and so
\[
\begin{aligned}
\frac{\tilde{\mu}_0 - \mu_0}{\tilde{\sigma}\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}} &= \frac{\tilde{\mu}_0 - \mu_0}{\sqrt{\frac{1}{n - 2}\sum_{i=1}^nE_i^2}\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}} \cdot \frac{\sigma}{\sigma} \\
&= \frac{\tilde{\mu}_0 - \mu_0}{\sigma\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}} \cdot\frac{\sigma}{\sqrt{\frac{1}{n - 2}\sum_{i=1}^nE_i^2}} \\
&= Z \cdot \sqrt{\frac{(n - 2)\sigma^2}{\sum_{i=1}^nE_i^2}}, Z = \frac{\tilde{\mu}_0 - \mu_0}{\sigma\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}} \sim N(0, 1) \\
&= \frac{Z}{\sqrt{\frac{\sum_{i=1}^nE_i^2}{\sigma^2}/(n - 2)}} \sim t_{n-2} \text{ by \hyperref[prop:117]{Proposition 1.1.7}(3)}.
\end{aligned}
\]
\indent By the symmetry of the \(t\)-distribution, we look for \(q_\alpha\) such that
\[
\begin{aligned}
\alpha &= \Pr\left(-q_\alpha \leq \frac{\hat{\mu}_0 - \mu_0}{\hat{\sigma}\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}} \leq q_\alpha\right) \\
&= \Pr\left(\hat{\mu}_0 - q_\alpha\hat{\sigma}\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}} \leq \mu_0 \leq \hat{\mu}_0 + q_\alpha\hat{\sigma}\sqrt{\frac1n + \frac{(x_0 - \bar{x})^2}{s_{xx}}}\right).
\end{aligned}
\]
\indent By this we get
\[
\alpha = 1 - 2\Pr(T \geq q_\alpha) = 1 - 2(1 - \Pr(T \leq q_\alpha)), T \sim t_{n - 2},
\]
which yields
\[
\Pr(T \leq q_\alpha) = 1 - \frac12(1 - \alpha), T \sim t_{n - 2},
\]
completing the proof.
\end{proof}

\begin{proposition} \label{prop:135}
\index{Simple linear regression!Confidence interval}
With the same setup as \hyperref[def:131]{Def. 1.3.1}, we have
\[
\tilde{y}_{new} - y_{new} = N\left(0, \sigma^2\left(1 + \frac1n + \frac{(x_{new} - \bar{x})^2}{s_{xx}}\right)\right).
\]
\end{proposition}
\begin{proof} By the linearity of expectation and the model assumption,
\[
\begin{aligned}
E(\tilde{y}_{new} - y_{new}) &= E(\tilde{\beta}_0 + \tilde{\beta}_1x_{new} - \beta_0 - \beta_1x_{new} - \ep_{new}) \\
&= E(\tilde{\beta}_0) + E(\tilde{\beta}_1x_{new}) - \beta_0 - \beta_1x_{new} - E(\ep_{new}) \\
&= \beta_0 + \beta_1x_{new} - \beta_0 - \beta_1x_{new} - 0 \\
&= 0.
\end{aligned}
\]
\indent Now observe that \(\tilde{y}_{new} = \tilde{\beta}_0 + \tilde{\beta}_1x_{new}\) is a linear combination of \(y_i\)'s, \(1 \leq i \leq n\), as pointed out in the proof of \hyperref[prop:132]{Proposition 1.3.2}, so \(\tilde{y}_{new} \perp y_{new}\), and so
\[
\begin{aligned}
\var(\tilde{y}_{new} - y_{new}) &= \var(\tilde{\beta}_0 + \tilde{\beta}_1x_{new}) - \var(y_{new}) \\
&= \sigma^2\left(\frac1n + \frac{(x_{new} - \bar{x})^2}{s_{xx}}\right) + \sigma^2 \\
&= \sigma^2\left(1 + \frac1n + \frac{(x_{new} - \bar{x})^2}{s_{xx}}\right).
\end{aligned}
\]
\indent Thus \(\tilde{y}_{new} - y_{new} \sim N\left(0, \sigma^2\left(1 + \frac1n + \frac{(x_{new} - \bar{x})^2}{s_{xx}}\right)\right)\) as required.
\end{proof}

\begin{proposition} \label{prop:136}
\index{Simple linear regression!Confidence interval}
With the same setup as \hyperref[def:131]{Def. 1.3.1}, \(\alpha \in [0, 1)\), then
\begin{enumerate}
\item If the population variance \(\sigma^2\) is known, then the \(100\alpha\%\) confidence interval of \(y_{new}\) given \(x_{new}\) is
\[
\left[\hat{y}_{new} - q_\alpha\sigma\sqrt{1 + \frac1n + \frac{(x_{new} - \bar{x})^2}{s_{xx}}}, \hat{y}_{new} + q_\alpha\sigma\sqrt{1 + \frac1n + \frac{(x_{new} - \bar{x})^2}{s_{xx}}}\right]
\]
where \(q_\alpha = \Phi^{-1}\left(1 - \frac12(1 - \alpha)\right)\), \(\Phi\) being the distribution function of the standard normal distribution.
\item If the population variance \(\sigma^2\) is unknown, then the \(100\alpha\%\) confidence interval of \(y_{new}\) given \(x_{new}\) is
\[
\left[\hat{y}_{new} - q_\alpha\hat{\sigma}\sqrt{1 + \frac1n + \frac{(x_{new} - \bar{x})^2}{s_{xx}}}, \hat{y}_{new} + q_\alpha\hat{\sigma}\sqrt{1 + \frac1n + \frac{(x_{new} - \bar{x})^2}{s_{xx}}}\right]
\]
where \(\hat{\sigma} = \frac{1}{n - 2}\sum_{i=1}^ne_i^2\), and \(q_\alpha = \Psi_{n - 2}^{-1}\left(1 - \frac1n(1 - \alpha)\right)\), \(\Psi_{n - 2}\) being the distribution function of the \(t_{n - 2}\) distribution. 
\end{enumerate}
\end{proposition}
\begin{proof} Analogous to \hyperref[prop:134]{Proposition 1.3.4}, \hyperref[prop:135]{1.3.5}.
\end{proof}

%----------------------------------------------------------------------------------------
%	CHAPTER 2
%----------------------------------------------------------------------------------------

\chapter{Multiple Linear Regression}

%------------------------------------------------

\section{Pre-Requisites and Definitions}

\begin{proposition}[Matrix algebra] \label{prop:211}
\index{Matrix algebra}
Let \(C \in \bR_{m\times n}\), \(A, B \in \bR_{n \times n}\) for some \(m, n \in \bN\), then
\begin{enumerate}
\item \((AB)^T = B^TA^T\).
\item If \(B\) is not singular, i.e. \(\det(B) \neq 0\), then \(B^{-1}B = I \in \bR_{n\times n}\).
\item If \(AB\) is not singular, then \((AB)^{-1} = B^{-1}A^{-1}\).
\item If \(A^T\) is not singular, then \((A^T)^{-1} = (A^{-1})^T\).
\item Define the trace function
\[
\begin{aligned}
\tr: \bR^{n\times n} &\rightarrow \bR \\
[a_{ij}]_{ij} &\mapsto \sum_{i=1}^n a_{ii}
\end{aligned}
\]
then (5.1) \(\tr(A + \alpha B) = \tr(A) + \alpha\tr(B)\) for all \(\alpha \in \bR\), (5.2) \(\tr(A) = \tr(A^T)\), (5.3) \(\tr(AB) = \tr(BA)\).
\end{enumerate}
\end{proposition}
\begin{proof} We omit the proof.\end{proof}

\begin{proposition}[Calculus with matrices] \label{prop:212}
\index{Matrix calculus}
Suppose \(f:\bR^n\rightarrow\bR^n\) is a function and \(z = f(y)\), then
\begin{enumerate}
\item
\[
\frac{\partial z}{\partial y} =
\begin{bmatrix}
\frac{\partial z}{\partial y_1} & \frac{\partial z}{\partial y_2} & \ldots & \frac{\partial z}{\partial y_n}
\end{bmatrix}
\]
where \(y = \begin{bmatrix} y_1 & y_2 & \ldots & y_n\end{bmatrix}^T\), each \(y_i \in \bR\), \(1 \leq i \leq n\).
\item If \(f(y) = a^Ty\) for some \(a \in \bR^n\), then
\[
\frac{\partial f}{\partial y} = a.
\]
\item If \(f(y) = y^TAy\) for some matrix \(A \in \bR_{n\times n}\), then
\[
\frac{\partial f}{\partial y} = Ay + A^Ty.
\]
\indent In particular, if \(A\) is symmetric, then \(\frac{\partial f}{\partial y} = 2Ay = 2A^Ty\).
\end{enumerate}
\end{proposition}
\begin{proof} We omit the proof.\end{proof}

\begin{definition} \label{def:213}
\index{Variance-covariance matrix}
Let \(Y \in \bR^n\) be a random vector. The \textbf{expectation} of \(Y\) is
\[
E(Y) = (E(Y_1) \ldots E(Y_n))^T
\]
where \(Y_1, \ldots, Y_n\) are random variables. \\
\indent The \textbf{variance-covariance matrix} of \(Y\) is
\[
\bR_{n\times n} \ni V := \var(Y) = \begin{bmatrix}
\var(Y_1) & \cov(Y_1, Y_2) & \ldots & \cov(Y_1, Y_n) \\
\cov(Y_2, Y_1) & \var(Y_2) & \ldots & \cov(Y_2, Y_n) \\
\vdots & \vdots & \ddots & \vdots \\
\cov(Y_n, Y_1) & \cov(Y_n, Y_2) & \ldots & \var(Y_n)
\end{bmatrix}
\]
which is derived from the definition
\[
\var(Y) = E((Y - \mu)(Y - \mu)^T), Y, \mu \in \bR^n
\]
where \(\mu = (E(Y_1) \ldots E(Y_n))^T\).
\end{definition}

\begin{proposition} \label{prop:214}
Let \(Y \in \bR^n\) be a random vector, then
\begin{enumerate}
\item \(\var(Y) = E(YY^T) - E(Y)E(Y)^T\).
\item \(\var(aY) = a\var(Y)a^T\) for constant \(a \in \bR^n\).
\item \(\var(Y + b) = \var(Y)\) for all \(b \in \bR^n\).
\end{enumerate}
\end{proposition}
\begin{proof} \begin{enumerate}
\item From definition
\[
\begin{aligned}
\var(Y) &= E((Y - \mu)(Y - \mu)^T) \\
&= E(YY^T - \mu Y^T - Y\mu^T + \mu\mu^T) \\
&= E(YY^T) - \mu E(Y^T) - E(Y)\mu^T + \mu\mu^T \text{ where } E(Y) = \mu \\
&= E(YY^T) - E(Y)E(Y^T).
\end{aligned}
\]
\item From definition
\[
\begin{aligned}
\var(aY) &= E((aY - E(aY))(aY - E(aY))^T) \\
&= E((aY - aE(Y))(aY - aE(Y))^T) \\
&= E(a(Y - E(Y))(Y - E(Y))^Ta^T) \\
&= aE((Y - E(Y))(Y - E(Y))^T)a^T \\
&= a\var(Y)a^T.
\end{aligned}
\]
\item From definition
\[
\begin{aligned}
\var(Y + b) &= E((Y + b - E(Y + b))(Y + b - E(Y + b))^T) \\
&= E((Y + b - E(Y) - b)(Y + b - E(Y) - b)^T) \\
&= E((Y - E(Y))(Y - E(Y))^T) \\
&= \var(Y).
\end{aligned}
\]
\end{enumerate}
\end{proof}

\begin{corollary} \label{cor:215}
Let \(Y \in \bR^n\) be a random vector, then \(\var(Y)\) is semi-positive definite, i.e. for all \(a \in \bR^n\), each term of the matrix \(a\var(Y)a^T\) is nonnegative.
\end{corollary}
\begin{proof} From \hyperref[prop:214]{Proposition 2.1.4}(2), \(a\var(Y)a^T = \var(aY)\) for all \(a \in \bR\), and all terms of a variance-covariance matrix, by the definition of variance and covariance in the random variable case, must be nonnegative.
\end{proof}

\begin{proposition} \label{prop:216}
Suppose \(Y = (Y_1, \ldots, Y_n)^T \in \bR^n\) is a random vector, \(a_1, \ldots, a_n \in \bR\), \(c_1, \ldots, c_n \in \bR\), \(b_1, \ldots, b_n \in \bR\), and \(d_1, \ldots, d_n \in \bR\) are constants, then
\[
\bR \ni Z := \sum_{i=1}^n a_iY_i + c_i, U := \sum_{i=1}^nb_iY_i + d_i
\]
are random variables with
\[
\begin{aligned}
E(Z) &= \sum_{i=1}^na_iE(Y_i) + c_i \\
E(U) &= \sum_{i=1}^nb_iE(Y_i) + d_i \\
\cov(Z, U) &= \sum_{i=1}^n\sum_{j=1}^n a_ib_j\cov(Y_i, Y_j).
\end{aligned}
\]
\indent In matrix notation, we have
\[
\begin{aligned}
\bR \ni &Z = a^TY + c, U = b^TY + d \\
&E(Z) = a^T\mu + c \in \bR \\
&\cov(Z, U) = a^T\var(Y)b \in \bR
\end{aligned}
\]
where \(\mu = (E(Y_1), \ldots, E(Y_n))^T \in \bR^n\), \(Y = (Y_1, \ldots, Y_n)^T \in \bR^n\), etc.
\end{proposition}
\begin{proof} This follows directly from the definitions of expectation, variance, and matrix arithmetic.\end{proof}

\begin{proposition} \label{prop:217}
Let \(Y \in \bR^n\) be a random vector and \(A \in \bR_{k\times n}\) be a matrix, then \(Z := (Z_1, \ldots, Z_k)^T\), defined by each 
\[
Z_i = \sum_{j=1}^n a_{ij}Y_j,
\]
is a random vector in \(\bR^k\), with
\[
\bR^k \ni E(Z) = AE(Y),
\]
\[
\bR^k \ni \var(Z) = A\var(Y)A^T.
\]
\indent In matrix notation, we have
\[
\bR^k \ni Z = AY.
\]
\end{proposition}
\begin{proof} We realise that
\[
Z_i = \sum_{j=1}^n a_{ij}Y_j
\]
for \(i = 1, \ldots, k\), so \(Z = AY\). The expectation result follows from \hyperref[prop:216]{Proposition 2.1.6}. For variance, we use its definition
\[
\begin{aligned}
\var(Z) = \var(AY) &= E((AY - E(AY))(AY - E(AY))^T) \\
&= E(A(Y - E(Y))(Y - E(Y))^TA^T) \\
&= AE((Y - E(Y))(Y - E(Y))^T) A^T \\
&= A\var(Y)A^T
\end{aligned}
\]
as required.
\end{proof}

\begin{definition} \label{def:218}
\index{Multivariate normal distribution}
\index{MVN|see{Multivariate normal distribution}}
\index{Population variance-covariance matrix}
Suppose \(Y \in \bR^n\) is a random vector, \(\mu, y \in \bR^n\), and \(\Sigma \in \bR_{n\times n}\) is a semi-positive definite symmetric matrix that is non-singular, then \(Y\) follows a \textbf{multivariate normal distribution} with population mean \(\mu\) and \textbf{population variance-covariance matrix} \(\Sigma\), written as
\[
Y \sim \mvnDist(\mu, \Sigma),
\]
if \(Y\) has the pdf
\[
f: y\mapsto \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac12}}\exp\left\{-\frac12(y - \mu)^T\Sigma^{-1}(y - \mu)\right\}
\]
where \(|\Sigma| = \det(\Sigma)\). \\
\indent We have
\[
[\Sigma]_{ij} = \cov(Y_i, Y_j), 1 \leq i \leq j \leq n.
\]
\end{definition}

\begin{proposition} \label{prop:219}
Suppose \(Z_1, \ldots, Z_n \sim N(0, 1)\) independently, \(A \in \bR_{n\times n}\) is a matrix, and \(\mu\in\bR^n\), then \(Y = AZ + \mu\) (\(Z := (Z_1, \ldots, Z_n)\)) has distribution
\[
Y \sim \mvnDist(\mu, \Sigma)
\]
where \(\Sigma = AA^T\) and \(\mu = E(Y)\).
\end{proposition}
\begin{proof} We omit the proof.\end{proof}

\begin{proposition} \label{prop:2110}
Suppose \(Y \in \bR^n\) is a random vector and \(Y \sim \mvnDist(\mu, \Sigma)\), then
\begin{enumerate}
\item If \(C \in \bR_{n\times n}\) and \(d \in \bR^n\) are constants, the random vector \(U := CY + d\) has
\[
U \sim \mvnDist(C\mu + d, C\Sigma C^T).
\]
\item If \(\tilde{Y} := (Y_1, \ldots, Y_m)^T\) for some \(m \leq n\), then
\[
\tilde{Y} \sim \mvnDist\left(
\begin{bmatrix} 
\mu_1 \\ \vdots \\ \mu_m
\end{bmatrix},
\begin{bmatrix}
\var(Y_1) & \ldots & \cov(Y_1, Y_m) \\
\vdots & \ddots & \vdots \\
\cov(Y_m, Y_1) & \ldots & \var(Y_m)
\end{bmatrix}
\right).
\]
\end{enumerate}
\end{proposition}
\begin{proof} We omit the proof.\end{proof}

\begin{proposition} \label{prop:2111}
Suppose \(Y \in \bR^n\) is a random vector with
\[
Y \sim \mvnDist(\mu, \Sigma)
\]
for some \(\mu \in \bR^n\) and \(\Sigma \in \bR_{n\times n}\), and let \(0 < p < n\), \(p \in \bN\). Define
\begin{enumerate}
\item \[Y_a = \begin{bmatrix} Y_1 \\ \vdots \\ Y_p\end{bmatrix}, Y_b = \begin{bmatrix} Y_{p+1} \\ \vdots \\ Y_n\end{bmatrix}\]
\item \[\mu_a = \begin{bmatrix} \mu_1 \\ \vdots \\ \mu_p\end{bmatrix}, \mu_b = \begin{bmatrix} \mu_{p+1} \\ \vdots \\ \mu_n\end{bmatrix}\]
\item \(\Sigma_{11}, \Sigma_{12}, \Sigma_{21}, \Sigma_{22}\) such that
\[
\begin{aligned}
\Sigma_{11} \in \bR_{p \times p}, &\Sigma_{12} \in \bR_{p\times(n-p)} \\
\Sigma_{21} \in \bR_{(n-p)\times p}, &\Sigma_{22}\in\bR_{(n-p)\times(n-p)}
\end{aligned}
\]
and
\[
\Sigma = \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{bmatrix},
\]
\end{enumerate}
then the conditional distribution of \(Y_a\) given \(Y_b = b\) for some constant \(b \in \bR^{n - p}\) is
\[
Y_a|Y_b = b \sim \mvnDist(\bar{\mu}, \bar{\Sigma})
\]
where
\[
\begin{aligned}
\bar{\mu} &= \mu_a + \Sigma_{12}\Sigma_{22}^{-1}(b - \mu_b) \\
\bar{\Sigma} &= \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}.
\end{aligned}
\]
\end{proposition}
\begin{proof} We omit the proof.\end{proof}

%------------------------------------------------

\section{Multiple Linear Regression}

%------------------------------------------------

\section{Inference}

%------------------------------------------------

\section{Prediction and Estimations}

%------------------------------------------------

\section{Categorical Variables}

%------------------------------------------------

\section{Interactions and Non-Linear Relationships}

%------------------------------------------------

\section{Analysis of Variance (ANOVA) and \(R^2\)}

%------------------------------------------------

\section{Collinearity}

%----------------------------------------------------------------------------------------
%	CHAPTER 3
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Model Building}

%----------------------------------------------------------------------------------------

\section{Goals and Criteria}

%----------------------------------------------------------------------------------------

\section{Model Selection}

%----------------------------------------------------------------------------------------

\section{LASSO and Shrinkage Methods}

%----------------------------------------------------------------------------------------
%	CHAPTER 4
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Model Diagnostics}

%----------------------------------------------------------------------------------------

\section{Residuals}

%----------------------------------------------------------------------------------------

\section{Fixing Models and Weighted Least Squared Method}

%----------------------------------------------------------------------------------------

\section{Outliers}


%------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

\cleardoublepage % Make sure the index starts on an odd (right side) page
\phantomsection
\setlength{\columnsep}{0.75cm} % Space between the 2 columns of the index
\addcontentsline{toc}{chapter}{\textcolor{ocre}{Index}} % Add an Index heading to the table of contents
\printindex % Output the index

%----------------------------------------------------------------------------------------

\end{document}
